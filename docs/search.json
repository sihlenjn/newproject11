[
  {
    "objectID": "addition.html",
    "href": "addition.html",
    "title": "Adding numbers in R",
    "section": "",
    "text": "To add numbers in R use +\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome page!",
    "section": "",
    "text": "This Website contains a paper written for a course on Data Science for Industry, the paper is include in the Page named “Main Assignment”. The main objective of the project is to fit at least three predictive models that take a sentence of text as input and return a prediction of which president was the source of that sentence. The following models are compared:\n\nneural networks\ndecision trees\nrandom forests\n\nThe stru"
  },
  {
    "objectID": "multiplication.html",
    "href": "multiplication.html",
    "title": "Data Science for Industry - Assignment 2",
    "section": "",
    "text": "Abstract\n\n\nIntroduction"
  },
  {
    "objectID": "multiplication.html#data",
    "href": "multiplication.html#data",
    "title": "Data Science for Industry - Assignment 2",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "assignment.html#data",
    "href": "assignment.html#data",
    "title": "Data Science for Industry Assignment 2",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "assignment.html",
    "href": "assignment.html",
    "title": "Data Science for Industry Assignment 2",
    "section": "",
    "text": "# Install all the required packages\nlibrary(kableExtra)"
  },
  {
    "objectID": "assignment.html#data-pre-processing",
    "href": "assignment.html#data-pre-processing",
    "title": "Data Science for Industry Assignment 2",
    "section": "Data pre processing",
    "text": "Data pre processing\nThe data set is loaded in R"
  },
  {
    "objectID": "assignment.html#exploratory-data-analysis",
    "href": "assignment.html#exploratory-data-analysis",
    "title": "Data Science for Industry Assignment 2",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nIn this study , the SONA data set will be used. This dataset contains all the speeches of the previous presidents from 1994 to 2023 which is in the form of text file. Pre processing of the data set is conducted so that we’re able to transform the dataset into the required form. The steps taken to clean and transform the data are indicated below.\n\n\n\n\n\nTop 10 mostly common used words by all presidents\n\n\n\n\nTop 10 most words used by each president.\n\n\n\n\n\nTop 10 most used words by each president\n\n\n\n\n\n\n\n2020 and 2021 Ramaphosa’s speeches\n\n\n\n\n\n\n\n\n\n\n\n\nBag of words model\nThe machine learning algorithms that will be applied in the paper accept a certain format of text data as they cannot operate on raw text. This means the data set must be represented in a numerical form prior being feed into a machine learning algorithm. Topper (n.d.) The bag-of-words model is used to for this , one can think of this as a method to convert words to numerical representation.\nIn this study, we are interested to tokenizer the data into sentences rather than words. To achieve this, the data set is tokenized using token 1 as sentences. And extra column, index is added which shows the index of the sentences in the data . The tibble is further tokenized into words. The bag of words model will be based on the top 1000 most used words by the presidents. AN\n\n\ntf-tdf model\nThis model measures the importance of a word to a document in a collection of documents ( Robinson (n.d.) ) This is measureed by how frequently a words occurs in a document, which is termed term frequency (tf). For the bag of words model, we need to consider whether to remove stop words or not. But this model uses the approach where it looks at a term’s inverse document frequency (idf), this decreases the weight for commonly used words and increase the weights for those words that are not commonly used in the document. This implies for this model, we do not have remove stop words. The inverse document frequency of any given term is defined as\n\nidf(\\text{term}) = \\ln \\left( \\frac{n_{\\text{docs}}}{n_{\\text{docs containing term}}} \\right)\n\\tag{1}\nwhere n_{\\text{docs}} is the number of documents. The bind_tf_idf() function available on the tidyverse library to convert our data set to this format. Equation 1 can be applied if one does the conversion form first principles , which is essentially what the function is performing."
  },
  {
    "objectID": "assignment.html#fitted-predictive-models",
    "href": "assignment.html#fitted-predictive-models",
    "title": "Data Science for Industry Assignment 2",
    "section": "Fitted predictive models",
    "text": "Fitted predictive models"
  },
  {
    "objectID": "assignment.html#predictive-models",
    "href": "assignment.html#predictive-models",
    "title": "Data Science for Industry Assignment 2",
    "section": "Predictive models",
    "text": "Predictive models\nTo fit the predictive models, different formats of the data sets will be used to access the performance of each model under each format. The bag-of-words format and the TF-IDF format and also other format that will take into account the issue of imbalance of the data as discussed in section ///. This aims to briefly explain the theory behind each model that will be fitted and also some packages and hyper-parameters that will used in our problem. The exact hyper-parameters are displayed in Section 4 . For example, would be number of trees to be used for the tree based models. In the section, we wo\nThe data set will be splitted into a training set and testing set. The training set will be used to train the dataset and performance of each model will assessed on the testing set. We used a 70/30 splitting rule, i.e. 70\\% of the data will be used as the training set and 30\\% as the testing set.\n\nNeural network\nIn this section , we fit a multi-layer perceptron Neural network. This network solves the shortcoming of the Feedforward Neural Network of not being able to learn through backpropagation. It is mainly used for classification problem which is exactly what we have. Since we have 6 presidents, this can be considered as a multiclassification problem with 6 classes. The fitting of the Neural Network is implemented using keras package in R. The parameters involved in fitting the model in keras are explained below:\nkeras_model_sequential() This part we specify the number of layers in our model and the activation function that each must have . The activation function used depends on the type of data set in question.\nThe most common used activation functions are sigmoid, ReLU and softmax .\nFor the base model, we used 3 layers. For the input layer, the ReLU activation function will be used with 201 units. And for the hidden layers, we use the softmax activation will be used for output layer and ReLU activation for the input layers.\n\ncompile()\nThis is the step one specify the loss function , optimizer and metrics. In our case, since this is a classification problem with 6 classes, the categorical_crossentropy is used. We use accuracy for metric and adam for the optimizer.\nTo fit the model, we specify the response variable and input variable and the number of epochs ."
  },
  {
    "objectID": "assignment.html#tree-based-methods",
    "href": "assignment.html#tree-based-methods",
    "title": "Data Science for Industry Assignment 2",
    "section": "Tree based methods",
    "text": "Tree based methods\nThe other predictive models that are considered are decision trees, which will include classification trees , random forests and gradient boosted trees. Each of these have their own advantage and disadvantage which will be explored below. The main aim to fit these tree method is to compare them to our neural network results. One of the disadvantanges of these methods is that the predictive accuracy of the trees is not as good as some other classification approaches.\nClassification trees\nClassification trees are applied to problems where the target variable is categorically which is relevant to our data set. At each step during the tree growth, a certain splitting criterion must be applied. The splitting criteria that exists in the literature are the Gini Index , Entropy and Deviance. In the paper, the Gini Index will be splitting criterion used for splitting the classification trees. The Gini Index measures the variability within the leaf nodes of a tree. At each step during tree growth, we choose a split that results in the greatest reduction of the Gini Index. The Gini Index is given by\n\nG =\\sum_{j=1}^J G_j \\quad \\text{where} \\quad  G_j = \\sum_{k=1}^K \\hat{p}_{jk}(1-\\hat{p}_{jk}\n)\n\\hat{p}_{jk} is the proportion of observations in the target variable k =1 ,..., K within leaf node j=1,…,J\nThis method will be implemented using the rpart() package in R with method = class since this is classification problem with 6 classes i.e. the number of presidents in the data set. The default splitting criteria used by this package is the Gini index.\nRandom Forests\nThis method uses bootstrapping sampling technique to grow trees on the bootstrap samples. The predictions are made by taking the majority vote (for classification problems) i.e. the prediction for an observation will be one occurring mostly among the 6 categories. The method also provides an improvement by decorrelating the trees that are produced on the boostrap samples. This will be implemented by using the randomForest package in R, the difference between a classification problem and regression problem in this package is the default value for number of trees used (mtry). For classification problem, \\text{mtry} \\approx \\sqrt{p} and for regression problem , \\text{mtry} = \\frac{p}{3} where p is the number of predictor variables in the data set.\nGradient Boosted Trees\nTuning parameters for the gbm package:\n\nB the number of trees to grow -\nLearning rate (\\lambda) -\nNumber of splits in each tree d -\n\nIn the methods stated above, one can explore the importance of the predictor variables used. However, our interest is to compare the performance of these models to the Neural Network in Section 3.3.1 section."
  },
  {
    "objectID": "assignment.html#neural-network",
    "href": "assignment.html#neural-network",
    "title": "Data Science for Industry Assignment 2",
    "section": "Neural network",
    "text": "Neural network"
  },
  {
    "objectID": "assignment.html#classification-trees",
    "href": "assignment.html#classification-trees",
    "title": "Data Science for Industry Assignment 2",
    "section": "Classification trees",
    "text": "Classification trees\n\n\nTable 1: Results of the classification tree model for different data formats\n\n\nData set\nAccuracy\n\n\n\n\nBag-of-words format\n\n\n\nTF-IDF format"
  },
  {
    "objectID": "assignment.html#random-forest",
    "href": "assignment.html#random-forest",
    "title": "Data Science for Industry Assignment 2",
    "section": "Random Forest",
    "text": "Random Forest\n\n\nTable 2: Results of the randomForest model for different data formats\n\n\nData set\nAccuracy\n\n\n\n\nBag-of-words format\n\n\n\nTF-IDF format"
  },
  {
    "objectID": "assignment.html#gradient-boosted-trees",
    "href": "assignment.html#gradient-boosted-trees",
    "title": "Data Science for Industry Assignment 2",
    "section": "Gradient Boosted trees",
    "text": "Gradient Boosted trees"
  }
]