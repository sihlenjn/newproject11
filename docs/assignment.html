<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sihle Njonga">

<title>Data Science for Industry Project - Data Science for Industry Assignment 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Data Science for Industry Project</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./assignment.html" aria-current="page">
 <span class="menu-text">Main assignment</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis">Exploratory Data Analysis</a>
  <ul class="collapse">
  <li><a href="#data-pre-processing" id="toc-data-pre-processing" class="nav-link" data-scroll-target="#data-pre-processing">Data pre processing</a></li>
  <li><a href="#data-analysis" id="toc-data-analysis" class="nav-link" data-scroll-target="#data-analysis">Data Analysis</a></li>
  <li><a href="#imbalanced-data" id="toc-imbalanced-data" class="nav-link" data-scroll-target="#imbalanced-data">Imbalanced data</a></li>
  </ul></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a>
  <ul class="collapse">
  <li><a href="#bag-of-words-model" id="toc-bag-of-words-model" class="nav-link" data-scroll-target="#bag-of-words-model">Bag of words model</a></li>
  <li><a href="#tf-tdf-model" id="toc-tf-tdf-model" class="nav-link" data-scroll-target="#tf-tdf-model">tf-tdf model</a></li>
  <li><a href="#predictive-models" id="toc-predictive-models" class="nav-link" data-scroll-target="#predictive-models">Predictive models</a>
  <ul class="collapse">
  <li><a href="#sec-neural-network" id="toc-sec-neural-network" class="nav-link" data-scroll-target="#sec-neural-network">Neural networks</a></li>
  <li><a href="#tree-based-methods" id="toc-tree-based-methods" class="nav-link" data-scroll-target="#tree-based-methods">Tree based methods</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-results" id="toc-sec-results" class="nav-link" data-scroll-target="#sec-results">Results</a>
  <ul class="collapse">
  <li><a href="#neural-network" id="toc-neural-network" class="nav-link" data-scroll-target="#neural-network">Neural network</a></li>
  <li><a href="#classification-trees" id="toc-classification-trees" class="nav-link" data-scroll-target="#classification-trees">Classification trees</a></li>
  <li><a href="#gradient-boosted-trees" id="toc-gradient-boosted-trees" class="nav-link" data-scroll-target="#gradient-boosted-trees">Gradient Boosted trees</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#list-of-packages-used-in-r" id="toc-list-of-packages-used-in-r" class="nav-link" data-scroll-target="#list-of-packages-used-in-r">List of packages used in <code>R</code></a></li>
  <li><a href="#data-accessibility" id="toc-data-accessibility" class="nav-link" data-scroll-target="#data-accessibility">Data accessibility</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Data Science for Industry Assignment 2</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    Sihle Njonga 
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Cape Town
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  

</header>

<div class="cell">

</div>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>This paper explores the application of different predictive models to text data, the predictive models applied are neural networks and tree based methods. We construct and compare three predictive models using different forms of the data set as our input. The aim is to build a model that take a sentence of text as input and return a prediction of which president was the source of that sentence. The data set used is the State of the Nation Address of the President of South African (SONA) speeches, which contains all the speeches from 1994 to 2023. Our findings through different explorations of the mentioned predictive models, with different parameters, the neural network models performed better the decision tree methods. However, the neural network returned an accuracy in the margins 0.5 - 0.55 on unseen data, which is quite low.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The objective of this paper is to construct at least three predictive models that takes a sentence of text as an input and return a prediction of which president was the source of that sentence. The data set that will be used is the State of the Nation Address of the President of South Africa (SONA). The data set contains speeches of all the SONA speeches from 1994 to 2023. This is a multi-classification problem with 6 classes since we had 6 presidents who have delivered speeches in the specified period.</p>
<p>Th paper is divided into three main sections namely Exploratory Data Analysis (EDA), Methodology and Results and Discussion. The overview of each section is as follows:</p>
<ul>
<li><p>Exploratory Data Analysis - this section contains the following sub-sections:</p>
<ul>
<li><p>Data pre-processing and data analysis - we provide information on how the data was cleaned and transformed so that we are conduct data analysis. The data analysis part includes an overview of the most commonly used words and bigrams in the speeches, we also take a closer look at president Cyril Ramaphosa’s 2020 and 2021 speeches.</p></li>
<li><p>Imbalanced data -in this section, analysis the average number of sentence used by each president in their speeches and see which president in under presented with respect to the length of their speech.</p></li>
</ul></li>
<li><p>Methodology - this section contains the following sub-sections:</p>
<ul>
<li><p>Bag of words model and TF-IDF model - The data set is converted to the two format which is what will be used the input in the predictive models that will be fitted.</p></li>
<li><p>Predictive models - In this sub-section, we provide a brief theory on each of the predictive models that will be fitted namely neural networks and tree based methods.</p></li>
</ul></li>
<li><p>Results and Conclusion - the results of the fitted models are provided in this section. The later section will compare the performance of the models on the different data sets used and any short comings. We also touch on some improvements that can be made to the analysis.</p></li>
</ul>
</section>
<section id="exploratory-data-analysis" class="level1">
<h1>Exploratory Data Analysis</h1>
<section id="data-pre-processing" class="level2">
<h2 class="anchored" data-anchor-id="data-pre-processing">Data pre processing</h2>
<p>Before we begin with the data cleaning, we load the data by the file names. The content (texts) of each speech is then loaded to an empty vector which is created prior the beginning of the cleaning process. It worth to note the individual speech contents are take from Ian’s github page and read as characters to the empty vector. The resulting data frame has two columns <em>filename</em> and <em>speech</em>. New columns, president, year and date are added to indicate from which president the speeches are from and also the dates the speeches took place. These are generated by using the file names since each file name contains the president names and the dates of speeches. Any unnecessary text is removed from the data set, these includes website links , encoding for new lines <code>\n</code> , backlashes and make sure all the dates are in the form <strong>dd-mm-yyyy</strong>. All the speeches being with the date on which they were delivered, we remove these in the data as they are not of interest. These dates are used to generate the dates column in the data frame , the dates are removed after the dates column has been generated. A new column, named <code>pres_num</code> ,is also included where each president name is denoted by integers from 1 to 6. All these data cleaning procedure are conducted using the <code>tidyverse</code> and <code>tidytext</code> packages.</p>
<div class="cell">

</div>
</section>
<section id="data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="data-analysis">Data Analysis</h2>
<p>In order to begin with EDA, we have to tokenize the data. As much we are interested in sentences, in our EDA will tokenize the data set by words and also sentences. This will give us insights on words that are commonly used by the presidents in their speeches.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-top10Words" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assignment_files/figure-html/fig-top10Words-1.png" class="img-fluid figure-img" width="2400"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Top 10 mostly common used words by all presidents.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><a href="#fig-top10Words">Figure&nbsp;1</a> depicts the top 10 most used words by the 6 presidents. In <a href="#fig-top10Words">Figure&nbsp;1</a> , we see that the words government , south , people and national are the most used words by the presidents. These words are indication of what the speeches are all about.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="assignment_files/figure-html/TopWordsIndiv-1.png" class="img-fluid" width="2400"></p>
</div>
</div>
<div class="cell">

</div>
<p>In <a href="#fig-ramaphosa_speeches">Figure&nbsp;2</a> and <strong>?@fig-ramaphosa_bigrams</strong> , we look at Ramaphosa’s speeches for the year 2020 and 2021. These speeches are of interest as they were made before the start of Covid-19 and after Covid-19. Upon analysing the data, sort the top words and bigrams used, we noticed that figures which were included in this speech were treated as individual words. For example, if the figure R250 000 was mentioned somewhere in the speech, R250 and 000 were treated as individual words. So after we extracted Ramaphosa’s 2020 and 2021 speeches, we cleaned the data to remove these space between the numbers. Before we did the cleaning, 000 was the most used ‘word’ in the speech for 2021 but not for 2020. This implies that a lot numbers were mentioned in Ramaphosa’s speech compared to his 2020 speech. There isn’t much difference between the words used in the 2020 and 2021 speeches in <a href="#fig-ramaphosa_speeches">Figure&nbsp;2</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ramaphosa_speeches" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assignment_files/figure-html/fig-ramaphosa_speeches-1.png" class="img-fluid figure-img" width="2400"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Comparison of the most used words by Ramaphosa for 2020 and 2021 speeches.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In Figure , this is where we notice a difference between the two speeches. As one would expect, the bi-grams Covid-19, million people and create jobs appears in the 2021 speech. The top 3 most used bi-grams are similar in both speeches.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assignment_files/figure-html/ramabigrams-1.png" class="img-fluid figure-img" width="2400"></p>
<p></p><figcaption class="figure-caption">Comparison of the most used bigrams by Ramaphosa for 2020 and 2021 speeches.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="imbalanced-data" class="level2">
<h2 class="anchored" data-anchor-id="imbalanced-data">Imbalanced data</h2>
<p>In <a href="#fig-average_sentences">Figure&nbsp;3</a> , the average number of sentence for each speech is displayed for each president. We can see that deKlerk’s speech had less number of sentences on average compared to the other presidents. As much as Motlanthe had only 1 speech, but the average number of sentence in his speech was in the same range as that of Zuma, Mbeki , Mandela and Ramaphosa.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-average_sentences" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assignment_files/figure-html/fig-average_sentences-1.png" class="img-fluid figure-img" width="2400"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Average number of sentences used by the presidents in their speeches.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>This creates an imbalance in the data set as deKlerk will be under represented in the data set. This means when creating training and test set, the class represented will have less frequency compared to the other presidents. To work around this when fitting the models, we have decided to create different data sets where the row with deKlerk’s speech is removed. We compare the performance to the data set when use the data set as it is.</p>
</section>
</section>
<section id="methodology" class="level1">
<h1>Methodology</h1>
<section id="bag-of-words-model" class="level2">
<h2 class="anchored" data-anchor-id="bag-of-words-model">Bag of words model</h2>
<div class="cell">

</div>
<p>The machine learning algorithms that will be applied in the paper accept a certain format of text data as they cannot operate on raw text. This means the data set must be represented in a numerical form prior being feed into a machine learning algorithm. <span class="citation" data-cites="blogBagModel">Topper (<a href="#ref-blogBagModel" role="doc-biblioref">n.d.</a>)</span> The bag-of-words model is used to for this , one can think of this as a method to convert words to a numerical representation.</p>
<p>In this paper, we are interested to tokenizer the data into sentences rather than words. To achieve this, the data set is tokenized using tokens as sentences. An extra column is then added which shows the index of each sentence in the data. This is done in order to be able to track from which sentence do the words come from. The tibble is then further tokenized into words. The bag of words model created is based on the top 1000 most used words by the presidents.</p>
</section>
<section id="tf-tdf-model" class="level2">
<h2 class="anchored" data-anchor-id="tf-tdf-model">tf-tdf model</h2>
<div class="cell">

</div>
<p>This model measures the importance of a word to a document in a collection of documents ( <span class="citation" data-cites="textMiningBook">Robinson (<a href="#ref-textMiningBook" role="doc-biblioref">n.d.</a>)</span> ). This is measured by how frequently a words occurs in a document, which is termed <em>term frequency (tf)</em>. For the bag of words model, we need to consider whether to remove stop words or not. But this model uses the approach where it looks at a term’s inverse document <em>frequency (idf),</em> this decreases the weight for commonly used words and increase the weights for those words that are not commonly used in the document. This implies for this model, we do not have remove stop words. Each term gets its own <span class="math inline">\text{tfidf}</span> score given by as</p>
<p><span id="eq-tfidt"><span class="math display">
tfidt(term) = tf(\text{term} \ t \ \text{in document}) \times idf(\text{term} \ t)
\tag{1}</span></span></p>
<p>where</p>
<p><span id="eq-idf"><span class="math display">
idf(\text{term  t}) = \ln \left( \frac{n_{\text{docs}}}{n_{\text{docs containing term $t$}}} \right)
\tag{2}</span></span> and <span id="eq-tf"><span class="math display">
tf(\text{term t in document})=\frac{\text{Number of times $t$ appears in document $i$}}{\text{Number of terms in document $i$}}
\tag{3}</span></span></p>
<p>where <span class="math inline">n_{\text{docs}}</span> is the number of documents. The <code>bind_tf_idf()</code> function available on the <code>tidyverse</code> library to convert our data set to this format. <a href="#eq-idf">Equation&nbsp;2</a> and <a href="#eq-tf">Equation&nbsp;3</a> can be applied if one does the conversion form first principles , which is essentially what the function is performing.</p>
<p>The data set is converted to this format by using the tokenized data where token is set to words. The models will be fitted in both bag of words format and TF-TDF format of the data sets and compare the results based on the performance on the test set.</p>
<p>To fit the predictive models, different formats of the data sets will be used to assess the performance of each model under each format. The bag-of-words format and the TF-IDF format and also other format that will take into account the issue of imbalance of the data.</p>
</section>
<section id="predictive-models" class="level2">
<h2 class="anchored" data-anchor-id="predictive-models">Predictive models</h2>
<p>This section aims to briefly explain the theory behind each model that will be fitted and also some packages and parameters that are used in each model. The exact parameters are shown in <a href="#sec-results">Section&nbsp;5</a> . This means, if we were to fit a tree based model that requires a certain number of trees as its parameters, th e exact value will be specified in <a href="#sec-results">Section&nbsp;5</a>.</p>
<p>The data set is splited into a training set and a testing set. The training set will be used to train the models and the performance of each model will assessed on the testing set. A 70/30 splitting rule is used in all the models, i.e.&nbsp;<span class="math inline">70\%</span> of the data will be used as the training set and <span class="math inline">30\%</span> as the testing set.</p>
<section id="sec-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="sec-neural-network">Neural networks</h3>
<p>In this section , we fit a multi-layer perceptron Neural network. It solves the shortcoming of the feedforward Neural Network of not being able to learn through back-propagation. It is mainly used for classification problems which is exactly what we have. Since we have 6 presidents, we are dealing with a multi-classification problem with 6 classes. The fitting of the Neural Network is implemented using <code>keras</code> package in <code>R</code>. The general framework for modelling using this keras is given below:</p>
<ul>
<li><p>Define the model. This entails specifying the layers in the models, the input layer and output layer will always be present as these refers predictor and target variables in the data set respectively. The input shape on the input layer has to correspond to the number of predictor variables in the training set. There is also an option to add drop out rate, a float between 0 and 1.This is a regularization technique that prevents the model from over-fitting.</p>
<p>Each layer must have its own activation function, this is determined by the type of data set, whether it’s continous or not. The most commonly used activation functions are SoftMax, ReLu and sigmoid.</p></li>
<li><p>The next step is to compile the model by specifying the loss function , the optimizer used and the metrics used to assess the performance of the model. For a multi-classification problem, the categorical_entropy since we have multiple classes.</p></li>
<li><p>The final step is fitting the model on the training set with a chosen number of epochs and the batch size. Predictions are then made on unseen data, the test set. The performance of the model is assessed based on the accuracy on the test set.</p></li>
</ul>
</section>
<section id="tree-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="tree-based-methods">Tree based methods</h3>
<p>The other predictive models that are considered are decision trees, which will include classification trees and gradient boosted trees. Each of these have their own advantage and disadvantage which will be explored below. The main aim to fit these tree method is to compare them to our neural network results. One of the disadvantages of these methods is that the predictive accuracy of the trees is not as good as some other classification approaches.</p>
<p><strong>Classification trees</strong></p>
<p>Classification trees are applied to problems where the target variable is categorically which is relevant to our data set. At each step during the tree growth, a certain splitting criterion must be applied. The splitting criteria that exists in the literature are the Gini Index , Entropy and Deviance. In the paper, the Gini Index will be splitting criterion used for splitting the classification trees. The Gini Index measures the variability within the leaf nodes of a tree. At each step during tree growth, we choose a split that results in the greatest reduction of the Gini Index. The Gini Index is given by</p>
<p><span id="eq-gini_index"><span class="math display">
G =\sum_{j=1}^J G_j \quad \text{where} \quad  G_j = \sum_{k=1}^K \hat{p}_{jk}(1-\hat{p}_{jk}
) \tag{4}</span></span></p>
<p><span class="math inline">\hat{p}_{jk}</span> is the proportion of observations in the target variable <span class="math inline">k =1 ,..., K</span> within leaf node <span class="math inline">j=1,…,J</span></p>
<p>This method will be implemented using the <code>rpart()</code> package in <code>R</code> with <code>method = class</code> since this is classification problem with 6 classes i.e.&nbsp;the number of presidents in the data set. The default splitting criteria used by this package is the Gini index.</p>
<p><strong>Gradient Boosted Trees</strong></p>
<p>The algorithm for Boosting is similar to that of Random Forest but the main different is that the trees are grown sequentially using the information from the previous trees. In RandomForests, each tree is built on a bootstrap sample independently of the other trees <span class="citation" data-cites="HastieGareth2013">James et al. (<a href="#ref-HastieGareth2013" role="doc-biblioref">2013</a>)</span> .The boosting algorithm does not involved boostrapp sampling , the algorithm has three main parameters:</p>
<p>Tuning parameters for the <code>gbm</code> package:</p>
<ul>
<li><p><span class="math inline">B</span> is the number of trees to grow - cross validation is used to select <span class="math inline">B</span> since a large value of <span class="math inline">B</span> may lead to over-fitting.</p></li>
<li><p>Learning rate (<span class="math inline">\lambda</span>) - typical values are 0.01 or 0.001, are generally used as choices for the learning rate. The same values will be implemented in this paper.</p></li>
<li><p>Number of splits in each tree <span class="math inline">d</span> - interaction depth of the model, the maximum number of splits the model has to perform on a tree. In most cases, <em>d</em> = 1 works well which impiles a single is considered.</p></li>
</ul>
<p>In the methods stated above, one can explore the importance of the predictor variables used. However, our interest is to compare the performance of these models to the Neural Network in <a href="#sec-neural-network">Section&nbsp;4.3.1</a> section. Hence, the importance of the variable used will not be explored.</p>
</section>
</section>
</section>
<section id="sec-results" class="level1">
<h1>Results</h1>
<section id="neural-network" class="level2">
<h2 class="anchored" data-anchor-id="neural-network">Neural network</h2>
<p>Four different models will be fitted. We start with a simple model, and then make modifications to the parameters as we move from one model to another to assess how the model perform which is determined by the accuracy of predictions of the test set. The models will be fitted on the original data set and also the data set where we have removed deKlerk’s speech. Throughout the paper, this model is referred to as the balanced data.The models fitted are indicated below:</p>
<ul>
<li><p><strong>Model 1</strong></p>
<ul>
<li><p>Number of hidden layers = 1 , activation function used RELu with 900 units.</p></li>
<li><p>Input layer : Activation function = ReLu , Units = number of rows in the training set, Inputshape = number of predictor variables.</p></li>
<li><p>Output layer : Activation function = SoftMax , units = 6 as we have 6 distinct lasses in the data set. The reason for using SoftMax it because we want to get probabilities since this is a classification problem.</p></li>
</ul></li>
<li><p><strong>Model 2</strong> This model has the same parameters as model 1 but a drop out regularization is introduce in the input layer and hidden layer to prevent over-fitting. The penalty applied is 0.05. The optimizer used for this model is <strong>rmsprop</strong>.</p></li>
<li><p><strong>Model 3</strong> This model has the same parameters as model 1, an extra hidden layer with 1020 units. The activation function used for the extra hidden layer is the sigmoid function.</p></li>
<li><p><strong>Model 4</strong> This model has the same parameters as model 3 but when fitting the model, the data will be scaled.</p></li>
</ul>
<p>In all the models where the activation function is not specified for the any of the layers, the input layer and the hidden layers uses the ReLU activation function and the output layer uses the SoftMax activation function was used. A batch size of 5 and 40 epochs was applied, there was no improvement in the accuracy of the model for large number of epochs hence a small value is used. (Refer to a figure in the appendix as evidence, this was for model 2)</p>
<div class="cell">

</div>
<div class="cell">

</div>
<div class="cell">

</div>
<div id="tbl-NN" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Results of the neural network models on the balanced bag of words data set</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>Accuracy (original data)</th>
<th>Accuracy (balance data)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>model 1</strong></td>
<td>0.5248</td>
<td>0.5325</td>
</tr>
<tr class="even">
<td><strong>model 2</strong></td>
<td>0.5382</td>
<td>0.5481</td>
</tr>
<tr class="odd">
<td><strong>model 3</strong></td>
<td>0.5326</td>
<td>0.5488</td>
</tr>
<tr class="even">
<td><strong>model 4</strong></td>
<td>0.5136</td>
<td>0.5092</td>
</tr>
</tbody>
</table>
</div>
<p><a href="#tbl-NN">Table&nbsp;1</a> shows the performance of the models on the out-of sample data. The model performs better on the data set where we have tried to balance the data by removing the speech of the president that had few sentence i.e.&nbsp;deKlerk’s speech. However, there’s no significant difference in the accuracy of all the models on both data set as they both produce an accuracy around 0.5 .</p>
<p>Looking at the individual models in the balanced data, model 2 and model 3 had a higher accuracy compared to the rest of the models. This implies that changing the optimizer and adding an extra hidden layer slightly improved the performance of the models.</p>
</section>
<section id="classification-trees" class="level2">
<h2 class="anchored" data-anchor-id="classification-trees">Classification trees</h2>
<div class="cell">

</div>
<p>To fit the classification trees, we use the rpart package in R. This model will the fitted on bag of words format and also the TF-IDF format. In both cases, we will consider when using the original data and also when using the balanced data. The results of for these models are indicated in <a href="#tbl-class_results">Table&nbsp;2</a> .</p>
<div id="tbl-class_results" class="anchored">
<table class="table">
<caption>Table&nbsp;2: Results of the classification tree model for different data formats</caption>
<thead>
<tr class="header">
<th>Data set</th>
<th>Data type used</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bag-of-words format</td>
<td>Original</td>
<td>0.295</td>
</tr>
<tr class="even">
<td>TF-IDF format</td>
<td>Original</td>
<td>0.295</td>
</tr>
<tr class="odd">
<td>Bag-of-words format</td>
<td>Balanced</td>
<td>0.281</td>
</tr>
<tr class="even">
<td>TF-IDF format</td>
<td>Balanced</td>
<td>0.281</td>
</tr>
</tbody>
</table>
</div>
<p>The models on the original data had better accuracy of 0.295 compared to the balanced data set. We also note that , the models attain the same accuracy irrespective of the format of the data that is used. This is cases for both the balanced and the original data. This implies that balancing the data did not make any significant improvement on the accuracy of the models. Similarly, whether we used the bag of words format or the TF-IDF format, we obtain the same accuracy of 0.295 and 0.281 respectively.</p>
</section>
<section id="gradient-boosted-trees" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosted-trees">Gradient Boosted trees</h2>
<div class="cell">

</div>
<div id="tbl-gbm_results" class="anchored">
<table class="table">
<caption>Table&nbsp;3: Results of the Gradient Boosted Tree model for different data formats</caption>
<thead>
<tr class="header">
<th>Data format</th>
<th>Data type used</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Comparing the overall results in the models fitted, the neural networks model performed better than the tree based based methods. We note that the performance of the neural networks on the different data sets varies as the parameters of the models are modified. This implies that it is still possible to get better results than the ones obtained in this paper as one tries to employ different modification to the parameters of the models. And the choice on the parameters to fix. The analysis was based on the top 1000 most common used words in the speeches, using more words could possibly yield different results although this is not guaranteed. We did not explore different numbers of words, this part was fixed.</p>
<p>With respect to the imbalance of the data set, only one option was considered to deal with data imbalance. This was assessing the average number of sentences of the speeches by each president. Then we decided to remove the president’s speech that was under represented. Our analysis was mainly based on using the two different data sets- the original data set and the balanced data set. Other techniques that could be considered included re-sampling techniques where the data is modified to balance the classes by oversampling and under-sampling. The second is SMOTE (Synthetic Minority Oversampling Technique) which creates synthetic examples for the under represented class in the unbalanced data.</p>
</section>




<div id="quarto-appendix" class="default"><section id="list-of-packages-used-in-r" class="level1 appendix"><h2 class="quarto-appendix-heading">List of packages used in <code>R</code></h2><div class="quarto-appendix-contents">

<p>In this section , we includes the main packages that we used in <code>R</code> and the corresponding sections in which the packages were used. The packages have also been cited under references. We only include the package names, not the individual functions that available under each package. These are shown in <a href="#tbl-packages">Table&nbsp;4</a> . [Remove this]</p>
<div id="tbl-packages" class="anchored">
<table class="table">
<caption>Table&nbsp;4: List of packages used in the study</caption>
<colgroup>
<col style="width: 27%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th>Package name</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>tidyverse</code></td>
<td>Data cleaning and pre-processing</td>
</tr>
<tr class="even">
<td><code>tidytext</code> <code>stingr</code></td>
<td>Manipulation of text data</td>
</tr>
<tr class="odd">
<td><code>keras</code></td>
<td>Fit a feed-forward neural network</td>
</tr>
<tr class="even">
<td><code>gridExtra</code></td>
<td>Organize multiple plots produced by the <code>ggplot</code> package to be one figure during EDA.</td>
</tr>
<tr class="odd">
<td><code>rpart</code> <code>gbm</code></td>
<td>Fitting tree based methods, classification tree , random forest and gradient boosted machines</td>
</tr>
</tbody>
</table>
</div>
</div></section><section id="data-accessibility" class="level1 appendix"><h2 class="quarto-appendix-heading">Data accessibility</h2><div class="quarto-appendix-contents">

<p>The original data set used for this study can be found in <a href="https://www.gov.za/state-nation-address">link</a> . We have also read the individual text files of the speeches for reading the data on <code>R</code> from Ian’s github page.&nbsp;All the analysis and fitting of models in this study will conducted using the free Statistical Computing software <code>R</code>. <span class="citation" data-cites="R">R Core Team (<a href="#ref-R" role="doc-biblioref">2023</a>)</span></p>
</div></section><section id="references" class="level1 appendix"><h2 class="quarto-appendix-heading"></h2><div class="quarto-appendix-contents">




</div></section><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-HastieGareth2013" class="csl-entry" role="doc-biblioentry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning with Applications in r</em>. Book. Vol. 103. Springer Texts in Statistics. New York: Springer.
</div>
<div id="ref-R" class="csl-entry" role="doc-biblioentry">
R Core Team. 2023. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-textMiningBook" class="csl-entry" role="doc-biblioentry">
Robinson, J. S. A. D. n.d. <span>“Welcome to <span>Text</span> <span>Mining</span> with <span>R</span> <span>Text</span> <span>Mining</span> with <span>R</span>.”</span> https://www.tidytextmining.com/.
</div>
<div id="ref-blogBagModel" class="csl-entry" role="doc-biblioentry">
Topper, Noah. n.d. <span>“Bag of Words Model in NLP Explained.”</span> https://www.builtin.com/machine-learning/bag-of-words.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>